---
title: Heuristic Search for Model Structure
abstact: 'Modern "machine learning" techniques can often discover useful patterns
  in high-dimensional data in a nearly automated fashion. These adaptive statistical
  algorithms -- including decision trees, regression networks, projection pursuit
  models, and additive network methods -- incrementally build up the model structure
  (inputs, interconnections, and terms) as well as set parameter values. That is,
  they sequentially add the component, from the set of candidates, which works best
  with the existing collection. This model expansion typically ceases when the structure
  is judged to optimally trade off 1 ) training accuracy and 2) simplicity (of model
  form or function surface). Complexity is regulated in order to protect against the
  serious danger of overfit, and thus lead to greater success on new data.  The heuristic
  search procedures employed by these methods are all "greedy" to some degree, as
  a consequence of making workable choices in an open or combinatorially huge domain
  of possible models. This procedure basically works well in practice, though it is
  known that, theoretically, greedy searches can result in arbitrarily worse models
  than optimal ones (when such are possible) on finite data sets. Some real and artificial
  examples of this will be shown for regression subset selection, decision trees,
  and artificial neural networks. ${ }^{2}$ It is useful then, to explore what restraining
  greed, or "lengthening the scoring horizon", can do for the methods. Toward this
  end, a recent decision tree algorithm, "Texas 2-Step", is described which looks
  two steps ahead to choose threshold variables and values, rather than one. (That
  is, it judges a split not by the purity of the resulting child nodes, but how the
  grandchildren turn out.) Preliminary results for some of these inductive methods
  are compared on a recent field application: identifying a bat''s species by its
  chirps.  In practice, greedily-constructed models sometimes outperform more optimal
  ones when tested on new data. We conclude by making preliminary suggestions about
  when this inversion is most likely to occur, and outline ways to improve the robustness
  of inductive procedures.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: elder95a
month: 0
tex_title: Heuristic Search for Model Structure
firstpage: 199
lastpage: 210
page: 199-210
order: 199
cycles: false
bibtex_author: Elder, IV, John F.
author:
- given: John F.
  family: Elder
  suffix: IV
date: 1995-01-05
note: Reissued by PMLR on 01 May 2022.
address:
container-title: Pre-proceedings of the Fifth International Workshop on Artificial
  Intelligence and Statistics
volume: R0
genre: inproceedings
issued:
  date-parts:
  - 1995
  - 1
  - 5
pdf: https://proceedings.mlr.press/r0/elder95a/elder95a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
